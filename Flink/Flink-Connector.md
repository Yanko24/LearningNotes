

###### 3. 其他参数

| 参数                                    |      是否必须      |     默认      |                  类型                  |                             描述                             |
| :-------------------------------------- | :----------------: | :-----------: | :------------------------------------: | :----------------------------------------------------------: |
| connector                               |      required      |    (none)     |                 String                 |                            kafka                             |
| topic                                   | required for sink  |    (none)     |                 String                 | 只可以为Source指定topic和topic-pattern之一，也可以支持topic list，多个topic之间使用分号分隔。Sink不支持主题列表 |
| topic-pattern                           |      optional      |    (none)     |                 String                 | 作业开始运行，Source将订阅topic和topic-pattern的所有主题，只能为源指定topic和topic-pattern之一 |
| properties.bootstrap.servers            |      required      |    (none)     |                 String                 |                      Kafka brokers列表                       |
| properties.group.id                     | required by source |    (none)     |                 String                 |                      kafka的消费者组ID                       |
| properties.*                            |      optional      |    (none)     |                 String                 |                                                              |
| format                                  |      required      |    (none)     |                 String                 | 用于反序列化和序列化kafka消息的值部分的格式，和value.format两者有一即可 |
| key.format                              |      optional      |    (none)     |                 String                 |                                                              |
| key.fields                              |      optional      |      []       |             List\<String\>             |                 定义key的显式列表，默认为空                  |
| key.fields-prefix                       |      optional      |    (none)     |                 String                 |                                                              |
| value.format                            |      required      |    (none)     |                 String                 | 用于反序列化和序列化kafka消息的值部分的格式，和format两者有一即可 |
| value.fields-include                    |      optional      |      ALL      | EnumPossible values: [ALL, EXCEPT_KEY] |                                                              |
| scan.startup.mode                       |      optional      | group-offsets |                 String                 | 对于kafka消费者的启动模式：earliest-offset/latest-offset/group-offsets/timestamp/specific-offsets |
| scan.startup.specific-offsets           |      optional      |    (none)     |                 String                 | 在specific-offsets的启动模式下为每个分区指定offset，例如：'partition:0,offset:21;partition:1,offset:98' |
| scan.startup.timestamp-millis           |      optional      |    (none)     |                  Long                  |            在timestamp启动模式下执行启动的时间戳             |
| scan.topic-partition-discovery.interval |      optional      |    (none)     |                Duration                |                                                              |
| sink.partitioner                        |      optional      |   'default'   |                 String                 |                                                              |
| sink.semantic                           |      optional      | at-least-once |                 String                 | 定义kafka-Sink的语义，默认是at-least_once，可以指定的有：at-least-once/exactly-once/none |
| sink.parallelism                        |      optional      |    (none)     |                Integer                 | 定义kafka-Sink的并行度，默认情况下并行度由上游流式运算的并行度来确定 |

##### 2. Formats

| Formats                                                      | Supported Connectors                                         |
| :----------------------------------------------------------- | :----------------------------------------------------------- |
| [CSV](https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/formats/csv.html) | [Apache Kafka](https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/kafka.html), [Upsert Kafka](https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/upsert-kafka.html), [Amazon Kinesis Data Streams](https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/kinesis.html), [Filesystem](https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/filesystem.html) |
| [JSON](https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/formats/json.html) | [Apache Kafka](https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/kafka.html), [Upsert Kafka](https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/upsert-kafka.html), [Amazon Kinesis Data Streams](https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/kinesis.html), [Filesystem](https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/filesystem.html), [Elasticsearch](https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/elasticsearch.html) |
| [Apache Avro](https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/formats/avro.html) | [Apache Kafka](https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/kafka.html), [Upsert Kafka](https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/upsert-kafka.html), [Amazon Kinesis Data Streams](https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/kinesis.html), [Filesystem](https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/filesystem.html) |
| [Confluent Avro](https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/formats/avro-confluent.html) | [Apache Kafka](https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/kafka.html), [Upsert Kafka](https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/upsert-kafka.html) |
| [Debezium CDC](https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/formats/debezium.html) | [Apache Kafka](https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/kafka.html), [Filesystem](https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/filesystem.html) |
| [Canal CDC](https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/formats/canal.html) | [Apache Kafka](https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/kafka.html), [Filesystem](https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/filesystem.html) |
| [Maxwell CDC](https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/formats/maxwell.html) | [Apache Kafka](https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/kafka.html), [Filesystem](https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/filesystem.html) |
| [Apache Parquet](https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/formats/parquet.html) | [Filesystem](https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/filesystem.html) |
| [Apache ORC](https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/formats/orc.html) | [Filesystem](https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/filesystem.html) |
| [Raw](https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/formats/raw.html) | [Apache Kafka](https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/kafka.html), [Upsert Kafka](https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/upsert-kafka.html), [Amazon Kinesis Data Streams](https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/kinesis.html), [Filesystem](https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/filesystem.html) |